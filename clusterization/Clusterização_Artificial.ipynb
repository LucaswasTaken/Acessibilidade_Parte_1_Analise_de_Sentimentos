{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "environment": {
      "name": "tf2-gpu.2-3.mnightly-2021-02-12-debian-10-test",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-02-12-debian-10-test"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Clusteriza√ß√£o Artificial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucaswasTaken/Acessibilidade_Parte_1_Analise_de_Sentimentos/blob/main/clusterization/Clusteriza%C3%A7%C3%A3o_Artificial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am2Wrz0uPnxG"
      },
      "source": [
        "# pytools setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NpnlfckAtVr"
      },
      "source": [
        "#@title Selecione o us√°rio: { form-width: \"50%\" }\n",
        "WORKSPACE = \"HT-BZS\" #@param [\"HT-Annoy\", \"HT-BZS\", \"development\", \"HT-BZS-JOAB\", \"HT-ASL\"] {allow-input: true}\n",
        "LANGUAGE_ID = 'ase' if WORKSPACE != 'HT-BZS' else 'bzs'\n",
        "user = \"lucas_1\" #@param ['lucas_1', 'lucas_2', 'ailton_1', 'ailton_2', 'lucas_cpp_1', 'lucas_cpp_2', 'lucas_cpp_3', 'lucas_cpp_4', 'denny_1', 'denny_2', 'wesley_1', 'wesley_2'] {allow-input: true}\n",
        "\n",
        "from copy import deepcopy\n",
        "from google.colab import auth,drive\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import json\n",
        "# Montar drive\n",
        "\n",
        "print('MONTAR DRIVE: ')\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Autentica√ß√£o √© necess√°ria para clonar\n",
        "\n",
        "print('AUTENTICAR: ')\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFAg7piXFwIG"
      },
      "source": [
        "%%capture\n",
        "!gcloud source repos clone github_hand-talk_pytools --project='ht-community' && pip install -r /content/github_hand-talk_pytools/requirements.txt\n",
        "sys.path.insert(0, '/content/github_hand-talk_pytools')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgVyZ4IQF2II"
      },
      "source": [
        "from handtalk.community import Community\n",
        "import handtalk.community as community\n",
        "import cereja as cj\n",
        "cmt = Community(WORKSPACE)\n",
        "\n",
        "# recuperando arquivos\n",
        "segments_to_extract = cj.FileIO.load(f'/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/PROCESSOS/Clusters_Artificiais/segments_to_extract_{WORKSPACE}.json').data[user]\n",
        "dict_of_prymary_segments = cj.FileIO.load(f'/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/PROCESSOS/Clusters_Artificiais/dict_of_prymary_segments_{WORKSPACE.split(\"-\")[1]}.json').data\n",
        "all_segment_sentences = cj.FileIO.load(f'/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/PROCESSOS/Clusters_Artificiais/all_segment_sentences_{WORKSPACE.split(\"-\")[1]}.json').data\n",
        "list_of_prymary_segments = cj.FileIO.load(f'/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/PROCESSOS/Clusters_Artificiais/list_of_prymary_segments_{WORKSPACE.split(\"-\")[1]}.txt').data\n",
        "if WORKSPACE == 'HT-BZS':\n",
        "    definitive_sentences = cj.FileIO.load(f'/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/PROCESSOS/Clusters_Artificiais/definitive_sentences_{WORKSPACE.split(\"-\")[1]}.json').data\n",
        "try:\n",
        "    next_index = int(cj.FileIO.load(f'/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/PROCESSOS/Clusters_Artificiais/results/testes/{WORKSPACE}/progress_{user}.txt').data[0])\n",
        "except:\n",
        "    next_index = 0\n",
        "\n",
        "try:\n",
        "    dict_signs_definite = cj.FileIO.load(f'/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/PROCESSOS/Clusters_Artificiais/results/testes/{WORKSPACE}/cluster_data_{WORKSPACE.split(\"-\")[1]}_{user}.json').data\n",
        "except:\n",
        "    dict_signs_definite = cj.FileIO.load(f'/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/PROCESSOS/Clusters_Artificiais/dict_signs_definite_{WORKSPACE.split(\"-\")[1]}.json').data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-HKLdHtXoZ1"
      },
      "source": [
        "# Resgatando dados do drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpN-UNCKFb3W"
      },
      "source": [
        "# Keypoints HT-BZS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCBfab0CFe3d"
      },
      "source": [
        "#carregando dados de Keypoints a serem usados\n",
        "%%capture\n",
        "!cp \"/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/MODELOS/SIAMESE/DATA/segments_keypoints_HT-BZS.zip\" ./\n",
        "!unzip /content/segments_keypoints_HT-BZS.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u7qXMxyFYn_"
      },
      "source": [
        "# Keypoins HT-ASL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S2tVtldXsBN"
      },
      "source": [
        "#carregando dados de Keypoints a serem usados\n",
        "# %%capture\n",
        "# !cp \"/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/MODELOS/SIAMESE/DATA/segments_keypoints_HT-ASL.zip\" ./\n",
        "# !unzip /content/segments_keypoints_HT-ASL.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyzzxYXyYQAt"
      },
      "source": [
        "# CARREGAR Rede siamesa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6tYz4Ss35rP"
      },
      "source": [
        "# Fun√ß√£√£o de Loss utilizada nas novas redes siamesas\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    y_true=tf.dtypes.cast(y_true, tf.float64)\n",
        "    y_pred=tf.dtypes.cast(y_pred, tf.float64)\n",
        "    margin = 1\n",
        "    square_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpPfPUgXYUSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cafefcb-e713-4f0f-822b-cd4d3cc12cf7"
      },
      "source": [
        "# carregando o modelo de rede siamesa. Atualmente utilizamos o \"model_siamese_test_lstm_v2.h5\"\n",
        "import tensorflow as tf\n",
        "#caminho para pasta dos modelos\n",
        "SEGMENTS_PATH = cj.Path('/content/content/segments_keypoints')\n",
        "MODEL = tf.keras.models.load_model('/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/MODELOS/CLASSIFICADOR_2_JOAB/model_siamese_test_new_rescale_fn_lstm_v2.5.h5',custom_objects={'contrastive_loss':contrastive_loss})\n",
        "print(\"Selecionado : model_siamese_test_new_rescale_fn_lstm_v2\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecionado : model_siamese_test_new_rescale_fn_lstm_v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4YU6Xv_0-e8"
      },
      "source": [
        "## Declara√ß√£√£o das fun√ß√µ√µes uteis a serem utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc5KgDztMzBh"
      },
      "source": [
        "# Essa √©√© uma fun√ß√£√£o especial, dados duas listas de segmentos pareados (Ids), ela fala o qu√£√£o similar cada par √©√©\n",
        "def siamese_predict_batch(a, b):\n",
        "    X1 = []\n",
        "    X2 = []\n",
        "    classes = []\n",
        "    for seg_id in a:\n",
        "        kpts = get_seg_kpts(seg_id)\n",
        "        if len(kpts) == 0:\n",
        "            continue\n",
        "        X1.append(kpts)\n",
        "    for seg_id in b:\n",
        "        kpts = get_seg_kpts(seg_id)\n",
        "        if len(kpts) == 0:\n",
        "            continue\n",
        "        X2.append(kpts)\n",
        "    X1 = np.array(X1)\n",
        "    X2 = np.array(X2)\n",
        "    \n",
        "    result = MODEL.predict([X1, X2])\n",
        "    return result.reshape(len(result))\n",
        "\n",
        "PRIMARY_SEGMENTS = None\n",
        "LAST_MIN_ON_CLUSTER = 5\n",
        "COUNT_SEGMENTS_ADDED = 0\n",
        "\n",
        "#PRINCIPAL FUN√á√É√ÉO D ETODAS, VARRE A PASTA DE KEYPIONTS E CARREGA O KEYPOINT DO SEGMENTO DE INTERESSE\n",
        "def get_seg_kpts(seg_id, seq_len=15):\n",
        "    global COUNT_SEGMENTS_ADDED\n",
        "    assert SEGMENTS_PATH.exists, 'Necess√°rio baixar todos os segmentos dos prim√°rios'\n",
        "    seg_file = SEGMENTS_PATH.join(f'{seg_id}.npy')\n",
        "    kpts = None\n",
        "    if seg_file.exists:\n",
        "        try:\n",
        "            kpts = np.load(seg_file)\n",
        "        except Exception as err:\n",
        "            print(f\"N√£o foi poss√≠vel resgatar keypoints do segmento {seg_id}: {err}\")\n",
        "            return []\n",
        "    else:\n",
        "        try:\n",
        "            kpts = cmt.get_document('segments', seg_id).keypoints\n",
        "            np.save(seg_file, kpts)\n",
        "            COUNT_SEGMENTS_ADDED += 1\n",
        "        except Exception as err:\n",
        "            print(f\"N√£o foi poss√≠vel resgatar keypoints do segmento {seg_id}: {err}\")\n",
        "    if kpts is None or len(kpts) == 0:\n",
        "        return []\n",
        "    return np.array(cj.rescale_values(kpts, seq_len, interpolation=True))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV-pdJ2QY6Vf"
      },
      "source": [
        "# Rodando algoritmo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8fkGNQzJnik",
        "outputId": "eb7b0a46-7515-48b7-83e1-4500647217a9"
      },
      "source": [
        "#Fun√ß√£o de similaridade entre senten√ßas\n",
        "import nltk\n",
        "nltk.download('rslp')\n",
        "from nltk.stem import RSLPStemmer\n",
        "stemmer = RSLPStemmer()\n",
        "def commom_words(sentence_1, sentence_2):\n",
        "    if WORKSPACE == 'HT-BZS':\n",
        "        sentence_1 = set([stemmer.stem(word) for word in sentence_1.split()])\n",
        "        sentence_2 = set([stemmer.stem(word) if word!='' else '' for word in sentence_2])\n",
        "    else:\n",
        "        sentence_1 = set([stemmer.stem(word) for word in sentence_1.split()])\n",
        "        sentence_2 = set([stemmer.stem(word) for word in sentence_2.split()])\n",
        "    return not sentence_1.isdisjoint(sentence_2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KysvsdImTzgD"
      },
      "source": [
        "dict_signs_definite['no_cluster'] = []"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQmHGlcuY9_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e028518d-1f8e-4f27-982c-4701d9cf29ed"
      },
      "source": [
        "from cereja import Progress\n",
        "import numpy as np\n",
        "list_of_prymary_segments = np.array(list_of_prymary_segments)\n",
        "count = 0\n",
        "for segment in Progress.prog(segments_to_extract):\n",
        "\n",
        "    # Calculando similaridade do segmento com os prim√°rios\n",
        "    list_segment = []\n",
        "    for i in range(len(list_of_prymary_segments)):\n",
        "        list_segment.append(segment)\n",
        "    list_segment = np.array(list_segment)\n",
        "    results = siamese_predict_batch(list_of_prymary_segments,list_segment)\n",
        "\n",
        "    #montando clusteres com as palavras relacionadas\n",
        "    chosen_arg = None\n",
        "    if WORKSPACE == 'HT-BZS':\n",
        "        for arg in np.argsort(results):\n",
        "            if results[arg]>= 0.3:\n",
        "                break\n",
        "            if commom_words(all_segment_sentences[segment],definitive_sentences[list_of_prymary_segments[arg]]):\n",
        "                chosen_arg = arg\n",
        "                break\n",
        "    else:\n",
        "        for arg in np.argsort(results):\n",
        "            if results[arg]>= 0.15:\n",
        "                break\n",
        "            if commom_words(all_segment_sentences[segment],all_segment_sentences[list_of_prymary_segments[arg]]):\n",
        "                chosen_arg = arg\n",
        "                break\n",
        "    # caso tenha enconteado palavras relacionadas com similaridade suficiente\n",
        "    if chosen_arg != None:\n",
        "        if results[chosen_arg] < 0.3:\n",
        "            closer_primary = list_of_prymary_segments[chosen_arg]\n",
        "            primary_sign = dict_of_prymary_segments[closer_primary]\n",
        "            dict_signs_definite[primary_sign].append(segment)\n",
        "        else:\n",
        "            dict_signs_definite['no_cluster'].append(segment)\n",
        "    else:\n",
        "        dict_signs_definite['no_cluster'].append(segment)\n",
        "    # salvando resultados\n",
        "    count+=1\n",
        "    if count%21 == 0:\n",
        "        cj.FileIO.create(f'/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/PROCESSOS/Clusters_Artificiais/results/testes/{WORKSPACE}/progress_{user}.txt',[count]).save(exist_ok=True)\n",
        "        cj.FileIO.create(f'/content/drive/Shareddrives/IA/CIENCIA_DE_DADOS/PROCESSOS/Clusters_Artificiais/results/testes/{WORKSPACE}/cluster_data_{WORKSPACE.split(\"-\")[1]}_{user}.json',dict_signs_definite).save(exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31müçí\u001b[34m Progress \u001b[36m¬ª\u001b[0;0m \u001b[0;0m00026/36189 - [\u001b[38;5;2m\u001b[0;0m\u001b[0;0m‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±‚ñ±] - 0.07% - üïú 00:13:08/16:39:49 estimated\u001b[0;0m \u001b[0;0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKvMC33b8NeZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}