{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "environment": {
      "name": "tf2-gpu.2-3.mnightly-2021-02-12-debian-10-test",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-02-12-debian-10-test"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Automatic_Match.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucaswasTaken/Acessibilidade_Parte_1_Analise_de_Sentimentos/blob/main/match_process/Automatic_Match.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB3YnhSdudYJ"
      },
      "source": [
        "### Caro usuário, esse notebook é destinado a popular a fila de matchs, bem como realizar os matchs automáticos se o usuário assim desejar. Para utiliza-lo, é necessário antes gerar uma fila de segementos para match com o notebook que a gera com a formatação adequada e treinar um modelo do Annoy e ter ele salvo no storage com seu respectivo tokenizador. Todos esses arquivos podem ser encontrados na pasta \"modelos\" do storage, no diretório raiz do comunity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdm3hM08UgBK"
      },
      "source": [
        "# Instalação do Pytools e Variaveis do Firebase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0gyUGQ2HNxZ"
      },
      "source": [
        "from google.colab import auth\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Autenticação é necessária para clonar\n",
        "auth.authenticate_user()\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulhfq8jjHhCW"
      },
      "source": [
        "%%capture\n",
        "!gcloud source repos clone github_hand-talk_pytools --project='ht-community' && pip install -r /content/github_hand-talk_pytools/requirements.txt\n",
        "sys.path.insert(0, '/content/github_hand-talk_pytools')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "KJ6aC15gHmuV",
        "outputId": "ffa18efc-3abc-4c6b-af0f-f002607ce81f"
      },
      "source": [
        "from handtalk.community import Community\n",
        "\n",
        "#@title Definição de variáveis globais  { form-width: \"30%\" }\n",
        "collection = \"segments\" #@param [\"animations\", \"videos\", \"contribs\", \"segments\", \"signs\", \"votes\"] {allow-input: true}\n",
        "WORKSPACE = \"HT-BZS\" #@param [\"HT-Annoy\", \"HT-BZS\", \"development\", \"HT-BZS-JOAB\", \"HT-ASL\"] {allow-input: true}\n",
        "cmt = Community(WORKSPACE)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31m🍒\u001b[30m Using Cereja v.1.3.7\r\n",
            "{'Faça o upload do arquivo json do service accounts do community. Se não tiver um, peça ao Thadeu.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9336fcc2-833c-4a93-9adb-51f85a0cb389\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9336fcc2-833c-4a93-9adb-51f85a0cb389\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ht-community-firebase-adminsdk-pazxx-a4c2b612bb.json to ht-community-firebase-adminsdk-pazxx-a4c2b612bb.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8jAPnBSHqA1"
      },
      "source": [
        "# Carregando dados do storage\n",
        "if WORKSPACE == 'HT-ASL':\n",
        "    cmt.storage.download(f'modelos/joab_model_class/asl/class_model_{WORKSPACE}.hdf5','./')\n",
        "    cmt.storage.download(f'modelos/joab_model_class/asl/class_model_{WORKSPACE}.txt','./')\n",
        "    cmt.storage.download(f'modelos/joab_model_class/asl/tokenizer_{WORKSPACE}.json','./')\n",
        "    cmt.storage.download(f'modelos/annoy/asl/{WORKSPACE}.json','./')\n",
        "    cmt.storage.download(f'modelos/annoy/asl/{WORKSPACE}.ann','./')\n",
        "    cmt.storage.download(f'modelos/siamese/siamese_keypoints1.h5','./')\n",
        "    cmt.storage.download(f'modelos/automatic_match/matched_{WORKSPACE}.txt','./')\n",
        "    cmt.storage.download(f'modelos/automatic_match/old_videos_{WORKSPACE}.txt','./')\n",
        "    cmt.storage.download(f'modelos/automatic_match/priority_segments_{WORKSPACE}.json','./')\n",
        "\n",
        "else:\n",
        "    cmt.storage.download(f'modelos/joab_model_class/bzs/class_model_{WORKSPACE}.hdf5','./')\n",
        "    cmt.storage.download(f'modelos/joab_model_class/bzs/tokenizer_{WORKSPACE}.json','./')\n",
        "    cmt.storage.download(f'modelos/annoy/bzs/{WORKSPACE}.json','./')\n",
        "    cmt.storage.download(f'modelos/annoy/bzs/{WORKSPACE}.ann','./')\n",
        "    cmt.storage.download(f'modelos/siamese/siamese_keypoints1.h5','./')\n",
        "    cmt.storage.download(f'modelos/automatic_match/matched_{WORKSPACE}.txt','./')\n",
        "    cmt.storage.download(f'modelos/automatic_match/priority_segments_{WORKSPACE}.json','./')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITPAEOnvHIvf"
      },
      "source": [
        "# Funções gerais e importaçã dos modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV258NhbHIvf"
      },
      "source": [
        "def rescale_list_ht(segment, granularity):\n",
        "        cluster = int(len(segment) / granularity)\n",
        "        if (cluster == 0):\n",
        "            multiplicador = int(granularity / len(segment) + 1)\n",
        "            oversampling = []\n",
        "\n",
        "            for array in segment:\n",
        "                for i in range(multiplicador):\n",
        "                    oversampling.append(array)\n",
        "\n",
        "            segment = oversampling\n",
        "            cluster = 1\n",
        "\n",
        "        flatten_result = []\n",
        "        start_interval = 0\n",
        "\n",
        "        for i in range(granularity):\n",
        "            frames = segment[start_interval:start_interval + cluster]\n",
        "            flatten_result.append(frames[-1])\n",
        "            start_interval += cluster\n",
        "        \n",
        "        assert len(flatten_result) == granularity, f\"Erro ao reescalar tamanho da lista {len(flatten_result)} != {granularity}\"\n",
        "        return flatten_result"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CARn96T8VCgF"
      },
      "source": [
        "### Rede siamesa do Lucas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_gYSUeSHIvg"
      },
      "source": [
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "import keras\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D\n",
        "from keras import backend as K\n",
        "\n",
        "def cosine_distance(vests):\n",
        "    x, y = vests\n",
        "    x = K.l2_normalize(x, axis=-1)\n",
        "    y = K.l2_normalize(y, axis=-1)\n",
        "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
        "\n",
        "def cos_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0],1)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def auroc(y_true, y_pred):\n",
        "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)\n",
        "\n",
        "siamese_model = load_model('/content/siamese_keypoints1.h5')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWwPnRdTHIvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946678dc-214a-49fd-c8c9-8df0e6a0f417"
      },
      "source": [
        "LANGUAGE = 'eng' if WORKSPACE != 'HT-BZS' else 'por'\n",
        "LANGUAGE_ID = 'ase' if LANGUAGE == 'eng' else 'bzs'\n",
        "print(f'Trabalhando em \"{WORKSPACE}\"')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trabalhando em \"HT-BZS\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_O8nRtsUsS9"
      },
      "source": [
        "### ANNOY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WryltSlmKDEo",
        "outputId": "9a651370-0ecb-4297-fa17-9a5146945112"
      },
      "source": [
        "!pip install annoy"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting annoy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz (646kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 5.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391566 sha256=91d665d9ed8e8b63936acd727cb3044c1ea906487096b03d993dbe08376ae115\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/c5/59/cce7e67b52c8e987389e53f917b6bb2a9d904a03246fadcb1e\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm1EwK-PHIvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb89dd0-37db-4bc4-be7d-f48847803087"
      },
      "source": [
        "from annoy import AnnoyIndex\n",
        "import random\n",
        "\n",
        "f = 840\n",
        "u = AnnoyIndex(f, 'manhattan')\n",
        "u.load(f'{WORKSPACE}.ann') # super fast, will just mmap the file"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLJuvV5DUwHl"
      },
      "source": [
        "### Funções do Modelo do Joab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDHysy5EHIvh"
      },
      "source": [
        "import json\n",
        "with open(f'{WORKSPACE}.json') as fi:\n",
        "    tokenizer = json.load(fi)\n",
        "\n",
        "detokenizer = {}\n",
        "for token in tokenizer:\n",
        "    detokenizer[tokenizer[token]] = token\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqmIRvXCHIvi"
      },
      "source": [
        "from handtalk.gcloud.firebase import Firebase, Firestore\n",
        "from handtalk.community import Community\n",
        "from copy import deepcopy\n",
        "from cereja import Progress, TfIdf\n",
        "import cereja as cj\n",
        "from typing import List, Set, Tuple, Dict\n",
        "p = Progress()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmCZ3m8yHIvi"
      },
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "Class for managing our data.\n",
        "\"\"\"\n",
        "import cereja as cj\n",
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import glob\n",
        "import os.path\n",
        "import sys\n",
        "import operator\n",
        "import threading\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "class threadsafe_iterator:\n",
        "    def __init__(self, iterator):\n",
        "        self.iterator = iterator\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        with self.lock:\n",
        "            return next(self.iterator)\n",
        "\n",
        "def threadsafe_generator(func):\n",
        "    \"\"\"Decorator\"\"\"\n",
        "    def gen(*a, **kw):\n",
        "        return threadsafe_iterator(func(*a, **kw))\n",
        "    return gen\n",
        "\n",
        "\n",
        "def process_image(image, target_shape=(299, 299, 3)):\n",
        "    img = load_img(image, target_size=target_shape[:2])\n",
        "    x = img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return x[0]\n",
        "\n",
        "class DataSet():\n",
        "\n",
        "    def __init__(self, data, classes, seq_length=10, image_shape=(299, 299, 3)):\n",
        "        \"\"\"Constructor.\n",
        "        seq_length = (int) the number of frames to consider\n",
        "        class_limit = (int) number of classes to limit the data to.\n",
        "            None = no limit.\n",
        "        \"\"\"\n",
        "        self.seq_length = seq_length\n",
        "        # self.model_features = Extractor('/content/drive/MyDrive/_video_frames_community/inception_best.hdf5')\n",
        "        \n",
        "        train_total = int(len(data) * 0.8)\n",
        "        random.shuffle(data)\n",
        "        self.test_data = data[train_total:]\n",
        "        \n",
        "        \n",
        "        self.train_data = data[:train_total]\n",
        "        self.data_length = len(self.train_data)\n",
        "        self.classes = sorted(classes)\n",
        "\n",
        "        self.image_shape = image_shape\n",
        "    \n",
        "\n",
        "    def get_class_one_hot(self, class_str):\n",
        "        \"\"\"Given a class as a string, return its number in the classes\n",
        "        list. This lets us encode and one-hot it for training.\"\"\"\n",
        "        # Encode it first.\n",
        "        label_encoded = self.classes.index(class_str)\n",
        "\n",
        "        # Now one-hot it.\n",
        "        label_hot = to_categorical(label_encoded, len(self.classes))\n",
        "\n",
        "        assert len(label_hot) == len(self.classes)\n",
        "\n",
        "        return label_hot\n",
        "        \n",
        "    @threadsafe_generator\n",
        "    def frame_generator_binary(self, batch_size, is_train=True, is_features=False):\n",
        "\n",
        "        while 1:\n",
        "            X1, X2, y = [], [], []\n",
        "            \n",
        "\n",
        "            # Generate batch_size samples.\n",
        "            for _ in range(batch_size):\n",
        "                # Reset to be safe.\n",
        "                a, b, c = None, None, None\n",
        "                \n",
        "                if is_train:\n",
        "                    a, b, c = random.choice(self.train_data)\n",
        "                else:\n",
        "                    a, b, c = random.choice(self.test_data)\n",
        "\n",
        "                if is_features:\n",
        "                    a = self.get_features(a)\n",
        "                    b = self.get_features(b)\n",
        "                else:\n",
        "                    # Get and resample frames.\n",
        "                    a = self.get_frames_for_sample(a)\n",
        "                    b = self.get_frames_for_sample(b)\n",
        "\n",
        "                    a = self.rescale_list_ht(a, self.seq_length)\n",
        "                    b = self.rescale_list_ht(b, self.seq_length)\n",
        "\n",
        "                    # Build the image sequence\n",
        "                    a = self.build_image_sequence(a)\n",
        "                    b = self.build_image_sequence(b)\n",
        "\n",
        "                X1.append(a)\n",
        "                X2.append(b)\n",
        "                y.append(c)\n",
        "            yield [np.array(X1), np.array(X2)], np.array(y)\n",
        "\n",
        "\n",
        "    @threadsafe_generator\n",
        "    def frame_generator_categorical(self, batch_size, is_train=True, is_features=False):\n",
        "\n",
        "        while 1:\n",
        "            X, y = [], []\n",
        "            \n",
        "\n",
        "            # Generate batch_size samples.\n",
        "            for _ in range(batch_size):\n",
        "                # Reset to be safe.\n",
        "                a, b = None, None\n",
        "                \n",
        "                if is_train:\n",
        "                    a, b = random.choice(self.train_data)\n",
        "                else:\n",
        "                    a, b = random.choice(self.test_data)\n",
        "\n",
        "                if is_features:\n",
        "                    a = self.get_features(a)\n",
        "                else:\n",
        "                    # Get and resample frames.\n",
        "                    a = self.get_frames_for_sample(a)\n",
        "\n",
        "                    a = self.rescale_list_ht(a, self.seq_length) # \n",
        "                    \n",
        "                    # Build the image sequence\n",
        "                    a = self.build_image_sequence(a)\n",
        "\n",
        "                X.append(a)\n",
        "                y.append(self.get_class_one_hot(b))\n",
        "            yield np.array(X), np.array(y)\n",
        "\n",
        "    def get_features(self, feature_path):\n",
        "        return np.load(feature_path)\n",
        "    \n",
        "    @classmethod\n",
        "    def rescale_list_ht(cls, segment, granularity):\n",
        "        if len(segment) >= granularity:\n",
        "            return cls.rescale_list(segment, granularity)\n",
        "        if (len(segment) == 0):\n",
        "            return []\n",
        "        cluster = int(len(segment) / granularity)\n",
        "        if (cluster == 0):\n",
        "            multiplicador = int(granularity / len(segment) + 1)\n",
        "            oversampling = []\n",
        "\n",
        "            for array in segment:\n",
        "                for i in range(multiplicador):\n",
        "                    oversampling.append(array)\n",
        "\n",
        "            segment = oversampling\n",
        "            cluster = 1\n",
        "\n",
        "        flatten_result = []\n",
        "        start_interval = 0\n",
        "\n",
        "        for i in range(granularity):\n",
        "            frames = segment[start_interval:start_interval + cluster]\n",
        "            flatten_result.append(frames[-1])\n",
        "            start_interval += cluster\n",
        "        \n",
        "        assert len(flatten_result) == granularity, f\"Erro ao reescalar tamanho da lista {len(flatten_result)} != {granularity}\"\n",
        "        return flatten_result\n",
        "\n",
        "\n",
        "    def build_image_sequence(self, frames):\n",
        "        \"\"\"Given a set of frames (filenames), build our sequence.\"\"\"\n",
        "        return [process_image(x, self.image_shape) for x in frames]\n",
        "\n",
        "\n",
        "    def get_frames_for_sample(self, sample):\n",
        "        \"\"\"Given a sample row from the data file, get all the corresponding frame\n",
        "        filenames.\"\"\"\n",
        "        return list(sorted(sample, key=lambda x: int(x.rsplit('_', 1)[-1].split('.')[0])))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def rescale_list(input_list, size):\n",
        "        \"\"\"Given a list and a size, return a rescaled/samples list. For example,\n",
        "        if we want a list of size 5 and we have a list of size 25, return a new\n",
        "        list of size five which is every 5th element of the origina list.\"\"\"\n",
        "        assert len(input_list) >= size, f'{len(input_list), size}'\n",
        "\n",
        "        # Get the number to skip between iterations.\n",
        "        skip = len(input_list) // size\n",
        "\n",
        "        # Build our new output.\n",
        "        output = [input_list[i] for i in range(0, len(input_list), skip)]\n",
        "\n",
        "        # Cut off the last one if needed.\n",
        "        return output[:size]\n",
        "\n",
        "\n",
        "    def class_from_prediction(self, predictions, nb_to_return=5):\n",
        "        \"\"\"Given a prediction, print the top classes.\"\"\"\n",
        "        # Get the prediction for each label.\n",
        "        label_predictions = {}\n",
        "        for i, label in enumerate(self.classes):\n",
        "            label_predictions[label] = predictions[i]\n",
        "\n",
        "        # Now sort them.\n",
        "        sorted_lps = sorted(\n",
        "            label_predictions.items(),\n",
        "            key=operator.itemgetter(1),\n",
        "            reverse=True\n",
        "        )\n",
        "        result = []\n",
        "        # And return the top N.\n",
        "        for i, class_prediction in enumerate(sorted_lps):\n",
        "            if i > nb_to_return - 1 or class_prediction[1] == 0.0:\n",
        "                break\n",
        "            result.append((class_prediction[0], class_prediction[1]))\n",
        "        return result"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3LSGcfBHIvo"
      },
      "source": [
        "## Resgatar lista de dados do firebase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gblro_hHIvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585a7ff2-7738-46cb-df66-3374414ff078"
      },
      "source": [
        "# atualize os snapshots dos segmentos aqui\n",
        "where = [('keypointsExtracted', '==', True)]\n",
        "videos_extracted = cmt.list_longer_collection('videos', where_config=where)\n",
        "len(videos_extracted)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31m🍒\u001b[34m Sys[out] \u001b[36m»\u001b[37m \u001b[37mTotal videos 30000 query time 26.74s\u001b[37m \u001b[37m\n",
            "\u001b[31m🍒\u001b[34m Sys[out] \u001b[36m»\u001b[37m \u001b[37mTotal videos 59677 query time 53.46s\u001b[37m \u001b[37m\n",
            "\u001b[31m🍒\u001b[34m Cereja \u001b[36m»\u001b[37m \u001b[37m[\u001b[38;5;2m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[37m] - 100.00% - 🕜 00:00:53 total - \u001b[38;5;2mDone! ✅\u001b[38;5;2m\u001b[37m \u001b[37m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59677"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2alD7BvHIvp"
      },
      "source": [
        "video_extracted_set = set(videos_extracted.keys())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccbMJaxtUCiO"
      },
      "source": [
        "if WORKSPACE == 'HT-ASL':\n",
        "    videos_old = set(cj.FileIO.load(f'/content/old_videos_{WORKSPACE}.txt').data)\n",
        "    segments_old = set(cj.FileIO.load(f'/content/matched_{WORKSPACE}.txt').data)\n",
        "    list_segments_old = list(cj.FileIO.load(f'/content/matched_{WORKSPACE}.txt').data)\n",
        "else:\n",
        "    segments_old = set(cj.FileIO.load(f'/content/matched_{WORKSPACE}.txt').data)\n",
        "    list_segments_old = list(cj.FileIO.load(f'/content/matched_{WORKSPACE}.txt').data)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti9pxi366Zo3"
      },
      "source": [
        "segments_to_match = cj.FileIO.load(f'/content/priority_segments_{WORKSPACE}.json').data"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1qG2LCEHIvp"
      },
      "source": [
        "# Definindo segmentos similares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqHy57aYU25l"
      },
      "source": [
        "### Antigo modelo do Joab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnR8Y_63HIvq"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "import numpy as np\n",
        "import cereja as cj\n",
        "import json\n",
        "\n",
        "# Carregando modelos\n",
        "\n",
        "if WORKSPACE == 'HT-ASL':\n",
        "    joab_model = load_model(\"/content/class_model_HT-ASL.hdf5\")\n",
        "\n",
        "    classes = cj.FileIO.load('/content/class_model_HT-ASL.txt').data\n",
        "    dataset = DataSet(\n",
        "        data=[],\n",
        "        classes=classes,\n",
        "        seq_length=10,\n",
        "        image_shape=(299, 299, 3)\n",
        "    )\n",
        "\n",
        "    with open(\"tokenizer_HT-ASL.json\") as output:\n",
        "        p_segments = json.load(output)\n",
        "\n",
        "else:\n",
        "    with open('/content/tokenizer_HT-BZS.json') as f:\n",
        "        joab_sign_dict = json.load(f)\n",
        "    joab_model = load_model(f\"/content/class_model_{WORKSPACE}.hdf5\")\n",
        "\n",
        "    with open(f\"tokenizer_{WORKSPACE}.json\") as output:\n",
        "        p_segments = json.load(output)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CerjnC8cHIvl"
      },
      "source": [
        "import keras\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import glob\n",
        "\n",
        "!mkdir temp_data\n",
        "\n",
        "def process_image(image, target_shape):\n",
        "    \"\"\"Given an image, process it and return the array.\"\"\"\n",
        "    # Load the image.\n",
        "    h, w, _ = target_shape\n",
        "    image = load_img(image, target_size=(h, w))\n",
        "\n",
        "    # Turn it into numpy, normalize and return.\n",
        "    img_arr = img_to_array(image)\n",
        "    x = np.asarray(img_arr / 255.).astype('float32')\n",
        "    return x\n",
        "\n",
        "# Modelo original do Joab pra indicar a que cluster o segmento pertence\n",
        "def predict_segment_cluster(segment_id,workspace):\n",
        "    if workspace == 'HT-ASL':\n",
        "        !rm -r temp_data/*\n",
        "        candidates = []\n",
        "        document = cmt.get_document('segments', segment_id)\n",
        "        document.save_frames('temp_data', (299, 299))\n",
        "        images = dataset.get_frames_for_sample(map(lambda x: x, glob.glob('/content/temp_data/*'))) # ordena frames\n",
        "        images = dataset.rescale_list_ht(images, 10)\n",
        "        images = dataset.build_image_sequence(images)\n",
        "        x = np.array([images])\n",
        "        prediction = joab_model.predict(x)\n",
        "        result = dataset.class_from_prediction(np.squeeze(prediction, axis=0), 1)\n",
        "        for sign_id, rate in result:\n",
        "            if rate > 0.8:\n",
        "                candidates.append([p_segments[sign_id],-0.99])\n",
        "                return candidates\n",
        "            elif rate > 0.6:\n",
        "                candidates.append([p_segments[sign_id],-0.90])\n",
        "                return candidates\n",
        "            else:\n",
        "                return candidates\n",
        "    else:\n",
        "        !rm -r ./temp_data/*\n",
        "        image_shape = (150, 150, 3)\n",
        "        cmt.get_document('segments', segment_id).save_frames('./temp_data/')\n",
        "        frames_path = [i for i in glob.glob('/content/temp_data/*')]\n",
        "        frames_path = list(sorted(frames_path, key=lambda x: int(x.rsplit('_')[-1].split('.')[0])))\n",
        "\n",
        "        frames = rescale_list_ht(frames_path, 10)\n",
        "        sample = np.array([[process_image(i, image_shape) for i in frames]])\n",
        "        \n",
        "        # Predict!\n",
        "        prediction = joab_model.predict(sample)\n",
        "        \n",
        "        candidates = [[joab_sign_dict[str(np.array(prediction[0]).argsort()[-1:][0])],-0.8]]\n",
        "        return candidates"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELS-QvNCX3hV"
      },
      "source": [
        "# Lista de segmentos presentes no modelo do Annoy\n",
        "list_annoy = set(detokenizer.keys())\n",
        "\n",
        "# Verifica membro commum em 2 frases\n",
        "def common_member(a, b): \n",
        "    a_set = set(a) \n",
        "    b_set = set(b) \n",
        "    if (a_set & b_set): \n",
        "        return True \n",
        "    else: \n",
        "        return False"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8rfrXYFQD86"
      },
      "source": [
        "# Template de voto automático\n",
        "from datetime import datetime\n",
        "\n",
        "vote_template = {\n",
        "    'createdAt' : datetime.now(),\n",
        "    'lastUpdate': datetime.now(),\n",
        "    'score': 2,\n",
        "    'votes': 1,\n",
        "    'automaticMatch': True,\n",
        "    'segment_a': '',\n",
        "    'segment_b': ''\n",
        "}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZKZVV8K4P1k"
      },
      "source": [
        "# Faz upload das atualizações para o storage\n",
        "def upload_automatic_segments(filepath: str, workspace: str):\n",
        "    blob = Community.firebase.storage.db.blob(f'modelos/automatic_match/matched_{workspace}.txt')\n",
        "    blob.upload_from_filename(filepath)\n",
        "\n",
        "    blob = Community.firebase.storage.db.blob(f'modelos/automatic_match/priority_segments_{workspace}.json')\n",
        "    blob.upload_from_filename(f'/content/priority_segments_{workspace}.json')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qued6Q2DCeCm"
      },
      "source": [
        "# Transforma em conjunto a lista de Match pra não haver repetição\n",
        "segments_list = set(segments_to_match.keys())"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQkeXtR-t7FY"
      },
      "source": [
        "## Formação da fila de match e matchs automáticos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "5T3qsYgdHIvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23ae125-abbb-48bb-ade6-c3abbb97ed35"
      },
      "source": [
        "from handtalk import utils\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "\n",
        "vote_col = cmt.firestore.db.collection('workspaces').document(WORKSPACE).collection('votes')\n",
        "false_match = {}\n",
        "count = 0\n",
        "segments_old = list(list_segments_old)\n",
        "want_auto_match= input('Matchar automaticamente? (sim ou nao) ')\n",
        "confidence= int(input('qual a confiança? (10-100) '))\n",
        "for seg_id in tqdm(segments_list):\n",
        "    # Segmento fora do annoy\n",
        "    if seg_id not in list_annoy:\n",
        "      continue\n",
        "    # Segmento ja matchado\n",
        "    if seg_id in segments_old:\n",
        "      continue\n",
        "\n",
        "    # Utilizando a rede do Joab com muita confiabilidade\n",
        "    try:\n",
        "        false_cluster = predict_segment_cluster(seg_id,WORKSPACE)\n",
        "    except:\n",
        "        false_cluster = []\n",
        "    # Utilziando rede siamesa do Lucas\n",
        "    candidates = {}\n",
        "    candidates['segment_similar'] = []\n",
        "\n",
        "    # Resgatar 50 segmentos similares do annoy\n",
        "    for segment_similar in u.get_nns_by_item(int(detokenizer[seg_id]), confidence+1):\n",
        "        if tokenizer[str(segment_similar)] == seg_id:\n",
        "            continue\n",
        "        candidates['segment_similar'].append(tokenizer[str(segment_similar)])\n",
        "    candidates['keypoints_similar'] = []\n",
        "\n",
        "    segment_data = cmt.get_document('segments',seg_id)\n",
        "    \n",
        "    # Resgatando Keypoints\n",
        "    keypoints = segment_data.video.keypoints_array[segment_data.start_frame:segment_data.end_frame]\n",
        "    actual_keypoint = [np.reshape(np.array(rescale_list_ht(keypoints, 10)),(10,177))]\n",
        "\n",
        "    for candidate in candidates['segment_similar']:\n",
        "        segment_data = cmt.get_document('segments',candidate)\n",
        "        keypoints = segment_data.video.keypoints_array[segment_data.start_frame:segment_data.end_frame]\n",
        "        candidates['keypoints_similar'].append(np.reshape(np.array(rescale_list_ht(keypoints, 10)),(10,177)))\n",
        "\n",
        "    results = []\n",
        "    actual_keypoint_list = []\n",
        "    for i in range(confidence):\n",
        "        actual_keypoint_list.append(actual_keypoint[0])\n",
        "    # Aplicando rede siamesa\n",
        "    results = -siamese_model.predict([np.array(actual_keypoint_list[0:confidence]),np.array(candidates['keypoints_similar'][0:confidence])])\n",
        "    results = np.reshape(results,(len(results)))\n",
        "\n",
        "    candidates['similares'] = false_cluster\n",
        "\n",
        "    # Aplicando rede siamesa\n",
        "    for seg_id_result in np.array(results).argsort()[0:10]:\n",
        "        candidates['similares'].append([candidates['segment_similar'][seg_id_result],results[seg_id_result]])\n",
        "        if len(candidates['similares']) == 10:\n",
        "            break\n",
        "    \n",
        "    false_match[seg_id] = candidates['similares']\n",
        "\n",
        "    list_segments_old.append(seg_id)\n",
        "    count+=1\n",
        "    clear_output(wait = True)\n",
        "    if count%50 == 0:\n",
        "        real_pairs = []\n",
        "        up_operations = []\n",
        "        \n",
        "        for seg_key in false_match:\n",
        "            if len(false_match[seg_key]) > 0:\n",
        "                main_sentence = utils.preprocess(cmt.get_document('segments',seg_key).video.sentence).split()\n",
        "                # aux_queue = True\n",
        "                if want_auto_match == 'sim':\n",
        "                    for sec_seg,score in false_match[seg_key][0:3]:\n",
        "                        sec_sentence = utils.preprocess(cmt.get_document('segments',sec_seg).video.sentence).split()\n",
        "                        if (common_member(main_sentence,sec_sentence)) and (-score > 0.985):\n",
        "                            real_pairs.append(sorted([seg_key,sec_seg]))\n",
        "                            aux_queue = False\n",
        "                # if aux_queue:\n",
        "                segments_to_match[seg_key]['similarSegments'] = [row[0] for row in false_match[seg_key]]\n",
        "\n",
        "        for linked in real_pairs:\n",
        "            to_link = sorted(linked)\n",
        "            id_doc = cmt.firestore.db.collection('whatever').document().id\n",
        "            vote_ref = vote_col.document(id_doc)\n",
        "            vote_data = deepcopy(vote_template)\n",
        "            vote_data['segment_a'] = to_link[0]\n",
        "            vote_data['segment_b'] = to_link[1]\n",
        "            vote_data['createdAt'] = datetime.now()\n",
        "            vote_data['lastUpdate'] = datetime.now()\n",
        "            up_operations.append((vote_ref, vote_data))\n",
        "\n",
        "\n",
        "        f=open(f'/content/matched_{WORKSPACE}.txt','w')\n",
        "        for ele in list_segments_old:\n",
        "            f.write(ele+'\\n')\n",
        "        f.close()\n",
        "        \n",
        "        with open(f'/content/priority_segments_{WORKSPACE}.json','w') as f:\n",
        "            json.dump(segments_to_match,f)\n",
        "\n",
        "        upload_automatic_segments(f'/content/matched_{WORKSPACE}.txt', WORKSPACE)\n",
        "\n",
        "        if len(up_operations) > 0:\n",
        "            cmt.firestore.set(up_operations,safe_mode=False)\n",
        "            # print(up_operations)\n",
        "            # cmt.firestore.set(up_operations)\n",
        "        false_match = {}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 638/19867 [2:08:19<88:34:50, 16.58s/it] "
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}